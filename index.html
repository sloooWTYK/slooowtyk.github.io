<!-- <!DOCTYPE html> -->
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Yuankai Teng, UofSC Math PhD Candidate</title>

    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- Le styles -->
    <link href="./assets_files/bootstrap.min.css" rel="stylesheet">
    <link href="./assets_files/bootstrap-responsive.min.css" rel="stylesheet">
    <link href="./assets_files/yangqing.css" rel="stylesheet">

    <!-- Le HTML5 shim, for IE6-8 support of HTML5 elements -->
    <!--[if lt IE 9]>
    <script src="assets/js/html5shiv.js"></script>
    <![endif]-->
    <link rel="icon" href="./myimg/2222.ico">
</head>

<div class="visible-phone" id="blackBar">
    <a href="#top">About</a>
    <!--<a href="#research">Research</a>-->
    <a href="#publications">Publications</a>
    <a href="#projects">Projects</a>
    <!--<a href="#teaching">Teaching</a>-->
    <a target="_blank"
       href="http://nbviewer.jupyter.org/github/sloooWTYK/docs/blob/master/Yuankai Teng CV.pdf">CV</a>
</div>

<body>

<div class="container span3 hidden-phone">
    <div id="floating_sidebar" class="span3">
        <!-- We use a fancy nav bar if there is enough space -->
        <!--<hr class="hidden-phone">-->
        <br>
        <ul class="nav nav-list bs-docs-sidenav hidden-phone">
            <li><a href="#top">About</a></li>
            <!--<li><a href="#research">Research</a></li>-->
            <li><a href="#publications">Publications</a></li>
            <li><a href="#projects">Projects</a></li>
            <!--<li><a href="#teaching">Teaching</a></li>-->
            <li><a target="_blank"
                   href="http://nbviewer.jupyter.org/github/sloooWTYK/docs/blob/master/Yuankai Teng CV.pdf">CV</a>
            </li>
        </ul>
        <hr class="hidden-phone">
        <div class="text-center hidden-phone">
            <img src="myimg/123.jpg" alt="photo" class="logo-image">
            <br><br>
            slooowtyk @ outlook.com <br>
        </div>

        <!-- Otherwise, we simply use a flat list of links -->

    </div>
</div>


<div class="container">

    <div class="row">

        <div class="span9">
            <br>
            <h3>
                Yuankai Teng (滕远凯)
            </h3>
            <h5>
                slooowtyk at outlook.com</a>
            </h5>
            <!-- Do I want to show a pic on the phone screen?
            <div class="text-center visible-phone">
                <img src="assets/img/Yihui.png" alt="photo" width="150px"/>
            </div>
            -->
            <a class="visible-phone pull-left" href="http://daggerfs.com/index.html#">
                <img class="media-object" src="myimg/123.jpg" width="96px" style="margin: 0px 10px">
            </a>
            <p>
                I am a third year PhD Candidate in the Department of Mathematics at the University of South Carolina. My advisors are <a target="_blank" href="https://people.math.sc.edu/ju/">Prof. Lili Ju</a> and <a target="_blank" href="https://people.math.sc.edu/wangzhu/">Prof. Zhu Wang</a>. Before that, I did my undergraduate studies at Wuhan University, where I worked with <a target="_blank" href="http://xpzhang.me/">Prof. Xiaoping Zhang</a>.
            </p>
            <p>
                My research focuses on Deep Learning based methods for PDE solution and function approximation. I will join Wells-Fargo as Quantitative Intern in summer 2022.
        <p> 
        </p>


            <!--
             *** Research ***
            -->
            <!--<h3>-->
            <!--<a name="research"></a> Research-->
            <!--</h3>-->
            <!--<p>-->
            <!--My current research topics include:-->
            <!--</p><ul>-->
            <!--<li> Learning better structures for image feature extraction.-->
            <!--</li><li> Explaining human generalization behavior with visually grounded cogscience models.-->
            <!--</li><li> Making large-scale vision feasible and affordable.-->
            <!--</li></ul>-->
            <!--<p></p>-->
            <!--<p> (Most recent publications to be added) </p>-->


            <!--
             *** Publications ***
            -->
            <h3>
                <a name="publications"></a> Publications
            </h3>
            <!--
            <p>
                Link to <a target="_blank"
                                           href="https://scholar.google.com/citations?user=z2w3scIAAAAJ&amp;hl=en"
                                           target="_blank">[Google Scholar]</a>
            </p>
            -->

        <div class="media">
                <a name="AMC" class="pull-left">
                    <img class="media-object" src="./myimg/Synthesized_regression.png" width="296px" height="296px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            Level Set Learning with Pseudo-Reversible Neural Networks for Nonlinear Dimension Reduction in Function Approximation
                     </strong><br>
                            <strong>Yuankai Teng</strong>, Zhu Wang, Lili Ju, Anthony Gruber, Guannan Zhang
                   <a target="_blank"
                           href="https://arxiv.org/pdf/2112.01438.pdf">[arXiv]</a>  
                    </p>
                    <p,class="abstract-text">
                        Due to the curse of dimensionality and the limitation on training data, approximating
high-dimensional functions is a very challenging task even for powerful deep neural networks. Inspired
by the Nonlinear Level set Learning (NLL) method that uses the reversible residual network (RevNet),
in this paper we propose a new method of Dimension Reduction via Learning Level Sets (DRiLLS) for
function approximation. Our method contains two major components: one is the pseudo-reversible
neural network (PRNN) module that effectively transforms high-dimensional input variables to
low-dimensional active variables, and the other is the synthesized regression module for approximating
function values based on the transformed data in the low-dimensional space. The PRNN not only
relaxes the invertibility constraint of the nonlinear transformation present in the NLL method due
to the use of RevNet, but also adaptively weights the influence of each sample and controls the
sensitivity of the function to the learned active variables. The synthesized regression uses Euclidean
distance in the input space to select neighboring samples, whose projections on the space of active
variables are used to perform local least-squares polynomial fitting. This helps to resolve numerical
oscillation issues present in traditional local and global regressions. Extensive experimental results
demonstrate that our DRiLLS method outperforms both the NLL and Active Subspace methods,
especially when the target function possesses critical points in the interior of its input domain.
                        </p>
                </div>
            </div> 
        
 <div class="media">
                <a name="AMC" class="pull-left">
                    <img class="media-object" src="./myimg/asG.jpg" width="296px" height="296px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            Learning Green's Functions of Linear Reaction-Diffusion Equations with Application to Fast Numerical Solver 
                     </strong><br>
                            <strong>Yuankai Teng</strong>, Xiaoping Zhang, Zhu Wang, Lili Ju
                        </p>
                       <p class="media-heading">
                        Mathematical and Scientific Machine Learning 2022.
                         <a target="_blank"
                           href="./myimg/MSML22_GFNet_Soft_github.pdf">[MSML22]</a>  
                        
                    </p>
                    <p,class="abstract-text">
                     Partial differential equations are commonly used to model various physical phenomena, such as
heat diffusion, wave propagation, fluid dynamics, elasticity, electrodynamics and so on. Due to
their tremendous applications in scientific and engineering research, many numerical methods have
been developed in past decades for efficient and accurate solutions of these equations on modern
computing systems. Inspired by the rapidly growing impact of deep learning techniques, we propose
in this paper a novel neural network method, “GF-Net”, for learning the Green’s functions of
the classic linear reaction-diffusion equation with Dirichlet boundary condition in the unsupervised
fashion. The proposed method overcomes the numerical challenges for finding the Green’s functions
of the equations on general domains by utilizing the physics-informed neural network and the
domain decomposition approach. As a consequence, it also leads to a fast numerical solver for the
target equation subject to arbitrarily given sources and boundary values without network retraining.
We numerically demonstrate the effectiveness of the proposed method by extensive experiments
with various domains and operator coefficients.   
                    </p>
                </div>
            </div> 
        
        
        <div class="media">
                <a name="AMC" class="pull-left">
                    <img class="media-object" src="./myimg/nll.png" width="296px" height="296px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            Nonlinear Level Set Learning for Function Approximation on Sparse Data with Applications to Parametric Differential Equations
                     </strong><br>
                            Anthony Gruber, Max Gunzburger, Lili Ju,<strong>Yuankai Teng</strong>, Zhu Wang 
                        </p>
                    <p class="media-heading">
                        Numer. Math. Theor. Meth. Appl. (2021).
                         <a target="_blank"
                           href="https://doi.org/10.4208/nmtma.OA-2021-0062">[DOI]</a>   
                    </p>
                    <p,class="abstract-text">
                        A dimension reduction method based on the "Nonlinear Level set Learning" (NLL) approach is presented for the pointwise prediction of functions which have been sparsely sampled. Leveraging geometric information provided by the Implicit Function Theorem, the proposed algorithm effectively reduces the input dimension to the theoretical lower bound with minor accuracy loss, providing a one-dimensional representation of the function which can be used for regression and sensitivity analysis. Experiments and applications are presented which compare this modified NLL with the original NLL and the Active Subspaces (AS) method. While accommodating sparse input data, the proposed algorithm is shown to train quickly and provide a much more accurate and informative reduction than either AS or the original NLL on two example functions with high-dimensional domains, as well as two state-dependent quantities depending on the solutions to parametric differential equations.
                    </p>
                </div>
            </div> 


       
        
        
                <div class="media">
                <a name="AMC" class="pull-left">
                    <img class="media-object" src="./myimg/boundary_precision_recall.png" width="296px" height="296px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            Interactive Binary Image Segmentation with Edge Preservation
                     </strong><br>
                         Jianfeng Zhang, Liezhuo Zhang,<strong>Yuankai Teng</strong>, Xiaoping Zhang, Song Wang, Lili Ju  
                         <a target="_blank"
                           href="https://arxiv.org/pdf/1809.03334.pdf">[arXiv]</a>  
                        
                    </p>
                    <p,class="abstract-text">
                        Binary image segmentation plays an important role in computer vision and has been widely used in many applications such as image and video editing, object extraction, and photo composition. In this paper, we propose a novel interactive binary image segmentation method based on the Markov Random Field (MRF) framework and the fast bilateral solver (FBS) technique. Specifically, we employ the geodesic distance component to build the unary term. To ensure both computation efficiency and effective responsiveness for interactive segmentation, superpixels are used in computing geodesic distances instead of pixels. Furthermore, we take a bilateral affinity approach for the pairwise term in order to preserve edge information and denoise. Through the alternating direction strategy, the MRF energy minimization problem is divided into two subproblems, which then can be easily solved by steepest gradient descent (SGD) and FBS respectively. Experimental results on the VGG interactive image segmentation dataset show that the proposed algorithm outperforms several state-of-the-art ones, and in particular, it can achieve satisfactory edge-smooth segmentation results even when the foreground and background color appearances are quite indistinctive.
                    </p>
                </div>
            </div>  
        <!--            
        <div class="media">
                <a name="cp" class="pull-left">
                    <img class="media-object" src="./assets_files/ill-1.png" width="96px" height="96px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                             Channel pruning for accelerating very deep neural networks
                     </strong><br>
                        <strong>Yihui He</strong>, Xiangyu Zhang, <a target="_blank" href="http://jiansun.org/">Jian Sun</a>, <strong>ICCV 2017</strong>
                        <a target="_blank"
                           href="http://openaccess.thecvf.com/content_iccv_2017/html/He_Channel_Pruning_for_ICCV_2017_paper.html">[PDF]</a>  
                        <a target="_blank"
                           href="https://arxiv.org/abs/1707.06168">[arXiv]</a>                          
                        <a target="_blank"
                           href="https://raw.githubusercontent.com/yihui-he/images/master/38722_He_0600.png">[Poster]</a>     
                        <a target="_blank"
                           href="http://nbviewer.jupyter.org/github/yihui-he/images/blob/master/channel-pruning-methods.pdf">[Slides]</a>                             
                        <a target="_blank"
                           href="https://github.com/yihui-he/channel-pruning">[Code]</a>       
                       
                    </p>
                    <p class="abstract-text">
                        In this paper, we introduce a new channel pruning method to accelerate very deep convolutional neural networks.Given a trained CNN model, we propose an iterative two-step algorithm to effectively prune each layer, by a LASSO regression based channel selection and least square reconstruction. We further generalize this algorithm to multi-layer and multi-branch cases. Our method reduces the accumulated error and enhance the compatibility with various architectures. Our pruned VGG-16 achieves the state-of-the-art results by 5x speed-up along with only 0.3% increase of error. More importantly, our method is able to accelerate modern networks like ResNet, Xception and suffers only 1.4%, 1.0% accuracy loss under 2x speed-up respectively, which is significant.
                    </p>
                </div>
            </div>   
                <div class="media">
                <a class="pull-left">
                    <img class="media-object" src="./assets_files/vehicle.png" width="96px" height="96px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>Vehicle Traffic Driven Camera Placement for Better Metropolis Security
                            Surveillance</strong><br>
                        Xiaobo Ma*, <strong>Yihui He*</strong>, Xiapu Luo, Jianfeng Li, Xiaohong Guan
                        , <strong>IEEE Intelligent System</strong>
                        <a target="_blank"
                           href="https://ieeexplore.ieee.org/abstract/document/8354911/">[PDF]</a>
                        <a target="_blank"
                           href="http://arxiv.org/abs/1705.08508">[arXiv]</a>
                        <a target="_blank"
                           href="https://github.com/yihui-he/Vehicle-Traffic-Driven-Camera-Placement">[Code]</a>                        
                    </p>
                    <p class="abstract-text">
                        Security surveillance is one of the most important issues in smart cities, especially in an era
                        of
                        terrorism. Deploying a number of (video) cameras is a
                        common approach for surveillance information retrieval.
                        Given the never-ending power offered by vehicles to a
                        metropolis, exploiting vehicle traffic to design camera
                        placement strategies could potentially facilitate physicalworld security surveillance. We take
                        the first step towards
                        exploring the linkage between vehicle traffic and camera
                        placement in favor of physical-world security surveillance
                        from a network perspective.
                    </p>
                </div>
            </div>
        <div class="media">
                <a class="pull-left">
                    <img class="media-object" src="./assets_files/filterwise.png" width="96px" height="96px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                             Pruning Very Deep Neural Network Channels for Efficient Inference
                     </strong><br>
                        <strong>Yihui He</strong>, Xiangyu Zhang, <a target="_blank" href="http://jiansun.org/">Jian Sun</a>
                        , <i>TPAMI, Major Revision</i>
                    </p>
                    <p class="abstract-text">
                        Channel Pruning is further expanded to Filterwise Pruning with rich experiements.
                    </p>
                </div>
            </div>
            <div class="media">
                <a class="pull-left">
                    <img class="media-object" src="./assets_files/superres.png" width="96px" height="96px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            Single Image Super-resolution with a Parameter Economic Residual-like Convolutional Neural Network
                        </strong><br>
                        Yudong Liang, Ze Yang, Kai Zhang, <strong>Yihui He</strong>, Jinjun Wang, Nanning Zheng, <i>TMM in submission</i>
                        <a target="_blank"
                           href="https://arxiv.org/abs/1703.08173">[arXiv]</a>
                    </p>
                    <p class="abstract-text">
                        This paper aims to extend the merits of residual network, such as skip
                        connection induced fast training, for a typical low-level vision problem,
                        i.e., single image super-resolution. In general, the two main challenges
                        of existing deep CNN for supper-resolution lie in the gradient exploding/vanishing
                        problem and large amount of parameters or computational
                        cost as CNN goes deeper. Correspondingly, the skip connections or identity
                        mapping shortcuts are utilized to avoid gradient exploding/vanishing
                        problem. To tackle with the second problem, a parameter economic CNN
                        architecture which has carefully designed width, depth and skip connections
                        was proposed. Different residual-like architectures for image superresolution
                        has also been compared. Experimental results have demonstrated
                        that the proposed CNN model can not only achieve state-of-the-art
                        PSNR and SSIM results for single image super-resolution but also produce
                        visually pleasant results.
                    </p>
                </div>
            </div>    
   
            <!--<div class="media">-->
            <!--<a class="pull-left" href="#top">-->
            <!--<img class="media-object" src="./assets_files/decaf-features.png" width="96px" height="96px">-->
            <!--</a>-->
            <!--<div class="media-body">-->
            <!--<p class="media-heading">-->
            <!--<strong>DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition</strong><br>-->
            <!--J Donahue, Y Jia, O Vinyals, J Hoffman, N Zhang, E Tzeng, T Darrell. arXiv preprint.<br>-->
            <!--<a target="_blank" href="http://arxiv.org/abs/1310.1531">[ArXiv Link]</a>-->
            <!--<a target="_blank" href="http://decaf.berkeleyvision.org/">[Live Demo]</a>-->
            <!--<a target="_blank" href="https://github.com/UCB-ICSI-Vision-Group/decaf-release/">[Software]</a>-->
            <!--<a target="_blank" href="http://www.eecs.berkeley.edu/~jiayq/decaf_pretrained/">[Pretrained ImageNet Model]</a>-->
            <!--</p>-->
            <!--<p class="abstract-text">-->
            <!--We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. We also released the software and pre-trained network to do large-scale image classification.-->
            <!--</p>-->
            <!--</div>-->
            <!--</div>-->

            <!--
             *** Projects ***
            -->

            <h3>
                <a name="projects"></a> Projects
            </h3>
            Link to my <a target="_blank" href="https://github.com/sloooWTYK">[github public projects]</a>

            <div class="media">
                <a class="pull-left">
                    <img class="media-object"
                         src="./myimg/ransac.jpg"
                         width="120px" height="120px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                                Low Frequency Objects In Unchanged Background Elimination Algorithm 
                        </strong>
                        <a target="_blank"
                           href="https://github.com/slooowTYK/RANSAC_python">[Code]</a>
                    </p>
                    <p class="abstract-text">
                        I use RANSAC(Random Sample Consensus) method to eliminate the useless objects which randomly show in a series time-continuous unchanged background images.
                        In this project I take the noise images as INPUT and give the modified clear background images as OUTPUT. The modified pictures will decline the noises which occurs in objects' 3D reconstruction by AI3D.
                        The code is available in my github.
                     </p>
                     <p class="abstract-text">
                        And this work is done during my internship in <i>Farsee2 Technology</i>.
                    </p>
                </div>
            </div>
<!--
<div class="media">
                <a class="pull-left">
                    <img class="media-object"
                         src="https://raw.githubusercontent.com/yihui-he/Depth-estimation-with-neural-network/master/presentation/stereo.png?token=AJkBS_A-YWaMd9vcgEQuaXQWe9wmjtTBks5XWM07wA%3D%3D"
                         width="96px" height="96px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            Estimated Depth Map Helps Image Classification
                        </strong>
                        <a target="_blank"
                           href="https://arxiv.org/pdf/1709.07077.pdf">[PDF]</a>
                        <a target="_blank" href="https://github.com/yihui-he/Estimated-Depth-Map-Helps-Image-Classification">[Code]</a>
                        <a target="_blank"
                           href="http://nbviewer.jupyter.org/github/yihui-he/Estimated-Depth-Map-Helps-Image-Classification/blob/master/presentation/depth.pdf">[Slides]</a>
                    </p>
                    <p class="abstract-text">
                        We consider image classification with estimated depth. This problem falls into the domain of transfer learning, since we are using a model trained on a set of depth images to generate depth maps (additional features) for use in another classification problem using another disjoint set of images. It's challenging as no direct depth information is provided. Though depth estimation has been well studied, none have attempted to aid image classification with estimated depth. Therefore, we present a way of transferring domain knowledge on depth estimation to a separate image classification task over a disjoint set of train, and test data. We build a RGBD dataset based on RGB dataset and do image classification on it. Then evaluation the performance of neural networks on the RGBD dataset compared to the RGB dataset. From our experiments, the benefit is significant with shallow and deep networks. It improves ResNet-20 by 0.55% and ResNet-56 by 0.53%.
                    </p>
                </div>
            </div>

            <div class="media">
                <a class="pull-left">
                    <img class="media-object"
                         src="https://www.kaggle.io/svf/310043/4567203286d71c8fd31cf12668a3ceac/__results___files/__results___5_0.png"
                         width="96px" height="96px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            Medical Image Segmentation: A survey
                        </strong>
                        <a target="_blank" href="https://github.com/yihui-he/u-net">[Code]</a>
                        <a target="_blank"
                           href="https://github.com/yihui-he/medical-image-segmentation-a-survey/blob/master/README.md">[Summary]</a>
                    </p>
                    <p class="abstract-text">
                        I evaluate DeepMask, Deeplab and MNC for medical image segmentation. I ranked
                        <strong>19%</strong> on <a target="_blank"
                                                   href="https://www.kaggle.com/c/ultrasound-nerve-segmentation/leaderboard/private">Kaggle
                        Ultrasound Nerve Segmentation</a>
                    </p>
                </div>
            </div>

            <div class="media">
                <a class="pull-left">
                    <img class="media-object"
                         src="https://raw.githubusercontent.com/yihui-he/deep-learning-guide/master/presentation/resnet.png"
                         width="96px" height="96px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>
                            residual neural network with tensorflow on CIFAR-100 
                        </strong>
                        <a target="_blank"
                           href="http://nbviewer.jupyter.org/github/yihui-he/ResNet-tensorflow/blob/master/report/mp2_Yihui%20He.pdf">[PDF]</a>
                        <a target="_blank" href="https://github.com/yihui-he/ResNet-tensorflow">[Code]</a>
                        <a target="_blank"
                           href="http://nbviewer.jupyter.org/github/yihui-he/deep-learning-guide/blob/master/presentation/mp12.pdf">[Slides]</a>
                    </p>
                    <p class="abstract-text">
                        I reimplement 6/13/20 layers RNN in tensorflow from scratch, and test it’s result on
                        CIFAR10/100.
                    </p>
                </div>
            </div>


            <div class="media">
                <a class="pull-left">
                    <img class="media-object"
                         src="https://raw.githubusercontent.com/yihui-he/deep-learning-guide/master/presentation/kmeans.jpg"
                         width="96px" height="96px">
                </a>

                <div class="media-body">
                    <p class="media-heading">
                        <strong>Single Layer neural network with PCAwhitening Kmeans</strong> 
                        <a target="_blank" href="http://nbviewer.jupyter.org/github/yihui-he/An-Analysis-of-Single-Layer-Networks-in-Unsupervised-Feature-Learning/blob/master/report/mp1_Yihui%20He.pdf">[PDF]</a>
                        <a target="_blank"
                                   href="https://github.com/yihui-he/An-Analysis-of-Single-Layer-Networks-in-Unsupervised-Feature-Learning">[Code]</a>
                        <a target="_blank"
                           href="http://nbviewer.jupyter.org/github/yihui-he/deep-learning-guide/blob/master/presentation/mp12.pdf">[Slides]</a>
                    </p>
                    <p class="abstract-text">
                        We evaluate whether features extracted from the activation of a deep convolutional network
                        trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be
                        re-purposed to novel generic tasks. We also released the software and pre-trained network to do
                        large-scale image classification.
                    </p>
                </div>
            </div>

            <div class="media">
                <a class="pull-left">
                    <img class="media-object"
                         src="https://raw.githubusercontent.com/yihui-he/my-Notes/master/person.jpg.png" width="96px"
                         height="96px">
                </a>

                <div class="media-body">
                    <p class="media-heading">
                        <strong>Objects Detection with YOLO on Artwork Dataset</strong> 
                        <a target="_blank" href="http://nbviewer.jupyter.org/github/yihui-he/Objects-Detection-with-YOLO-on-Artwork-Dataset/blob/master/Report_Yihui.pdf">[PDF]</a>
                        <a target="_blank"
                                   href="https://github.com/yihui-he/Objects-Detection-with-YOLO-on-Artwork-Dataset">[Code]</a>
                    </p>
                    <p class="abstract-text">
                        I design a small object detection network, which is simplified from YOLO(You Only Look Once)
                        network. It's trained on PASCAL VOC. I evaluate it on an artwork dataset(Picasso dataset). With
                        the best parameters, I got 40% precision and 35% recall.
                    </p>
                </div>
            </div>

            <div class="media">
                <a class="pull-left">
                    <img class="media-object" src="./assets_files/shuttle.png" width="96px" height="96px">
                </a>
                <div class="media-body">
                    <p class="media-heading">
                        <strong>shuttlecock detection and tracking</strong>
                        <a target="_blank"
                                   href="https://github.com/yihui-he/Badminton-Robot">[Code]</a>
                    </p>
                    <p class="abstract-text">
                        With guassian mixture model, I extract shuttlecock proposals. Then I use Partical filter to
                        refine proposals. From multi view cameras, I employed
                        structure from motion to predict its 3D location. Combined with Physics laws, landing location
                        prediction accuracy is around 5 cm. (This system
                        works on embeded linux with openCV)
                    </p>
                </div>
            </div>
-->
            
            <!-- Footer
            ================================================== -->
            <hr>
            <footer class="footer">
                <div class='hidden-phone'>
                <h3 class="text-center"><a name="wall"></a><strong>Photography Show</strong></h3>
                
                <a target="_blank" href="https://github.com/slooowTYK/docs"><img
                        src="https://github.com/sloooWTYK/docs/blob/master/491536653601_.pic.jpg?raw=true"></a>
                <hr>

                </div>
                <div class="row">
                    <div class="span12">

                        <p>
                            Modified By <a target="_blank" href="http://daggerfs.com/">© TonyTeng(Yuankai Teng) 2021</a>
                        </p>
                    </div>
                </div>

            </footer>
        </div>
    </div>
</div>
</body>
</html>

<!-- Le javascript
================================================== -->
<!-- Placed at the end of the document so the pages load faster -->
<!--
    <script type="text/javascript" src="http://platform.twitter.com/widgets.js"></script>
    <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
    <script src="assets/js/bootstrap-transition.js"></script>
    <script src="assets/js/bootstrap-alert.js"></script>
    <script src="assets/js/bootstrap-modal.js"></script>
    <script src="assets/js/bootstrap-dropdown.js"></script>
    <script src="assets/js/bootstrap-scrollspy.js"></script>
    <script src="assets/js/bootstrap-tab.js"></script>
    <script src="assets/js/bootstrap-tooltip.js"></script>
    <script src="assets/js/bootstrap-popover.js"></script>
    <script src="assets/js/bootstrap-button.js"></script>
    <script src="assets/js/bootstrap-collapse.js"></script>
    <script src="assets/js/bootstrap-carousel.js"></script>
    <script src="assets/js/bootstrap-typeahead.js"></script>
    <script src="assets/js/bootstrap-affix.js"></script>
    <script src="assets/js/holder/holder.js"></script>
    <script src="assets/js/application.js"></script>
-->
